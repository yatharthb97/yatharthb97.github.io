<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Cluster | Yatharth Bhasin</title>
<meta name=keywords content="blog,notes"><meta name=description content="Notes on how to set up a cluster."><meta name=author content="Yatharth Bhasin"><link rel=canonical href=https://yatharthb97.github.io/notes/distributedcomputing/cluster/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.296f1c593c14c1225e98bb70764ecc0ba42c9fb87f3b60196d6d71226602183b.css integrity="sha256-KW8cWTwUwSJemLtwdk7MC6Qsn7h/O2AZbW1xImYCGDs=" rel="preload stylesheet" as=style><link rel=icon href=https://yatharthb97.github.io/images/website_tile.svg><link rel=icon type=image/png sizes=16x16 href=https://yatharthb97.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://yatharthb97.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://yatharthb97.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://yatharthb97.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yatharthb97.github.io/notes/distributedcomputing/cluster/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><!doctype html><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css integrity=sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js integrity=sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!0})})</script><meta property="og:url" content="https://yatharthb97.github.io/notes/distributedcomputing/cluster/"><meta property="og:site_name" content="Yatharth Bhasin"><meta property="og:title" content="Cluster"><meta property="og:description" content="Notes on how to set up a cluster."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2025-03-25T17:37:01+00:00"><meta property="article:modified_time" content="2025-03-25T17:37:01+00:00"><meta property="article:tag" content="Blog"><meta property="article:tag" content="Notes"><meta property="og:image" content="https://yatharthb97.github.io/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yatharthb97.github.io/%3Cimage%20path/url%3E"><meta name=twitter:title content="Cluster"><meta name=twitter:description content="Notes on how to set up a cluster."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":" ‚úíÔ∏è ","item":"https://yatharthb97.github.io/notes/"},{"@type":"ListItem","position":2,"name":"Distributed Computing (üñ•Ô∏è X10000)","item":"https://yatharthb97.github.io/notes/distributedcomputing/"},{"@type":"ListItem","position":3,"name":"Cluster","item":"https://yatharthb97.github.io/notes/distributedcomputing/cluster/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Cluster","name":"Cluster","description":"Notes on how to set up a cluster.","keywords":["blog","notes"],"articleBody":"Introduction We will set-up a cluster using a job-scheduler (SLURM) and use dask-distributed to manage jobs. One does not normally choose the job-scheduler, it is given to us (like your institute or HPC service would use a specific one). It is for this reason that dask comes in handy. It implements a trivial-parallelisation model on top of most common job-schedulers (SLURM, PBS, etc) to name a few. The interface it provides resembles python‚Äôs multiprocessing module.\nAnother benefit is that using dask native arrays and dataframes allow you to automatically manage memeory and work with ‚Äúlarger-than-memory‚Äù objects. However, working with these objects is tricky and especially complicated when you need to use libraries written in C like python-opencv. If you can get it to work, here is a good article that deals with a typical workflow: here and here.\nWe will only use it dask-distributed to schedule jobs on the fly using jupyter-lab. This approach allows us to do batch analysis before computation and allows us to compute parameters and send jobs on the fly. The interface is the same as multiprocessing.fututes and general multiprocessing rules are respected.\nTo efficiently compute on HPC machines using these two libraries, we need to understand:\nSLURM resource managements dask-distributed resource managements Patching and common pitfalls We will not cover aspects like installation and debugging here.\n1. SLURM resource management User ‚Üí Submits ‚Üí Job ‚Üí Runs in ‚Üí Partition ‚Üí Contains ‚Üí Nodes\nJob ‚Üí Spawns ‚Üí Steps\nJob ‚Üí Uses ‚Üí Licenses\nQoS ‚Üí Governs ‚Üí Job Limits\nReservation ‚Üí Reserves ‚Üí Nodes\nhence we know that each ‚Äújob‚Äù can do several ‚Äútasks‚Äù:\nJobs (With a JobID :: this corresponds to one dask worker) |- Task1 |- Task2 : |- TaskN We need to properly allocate three things: physical-cpu-core, threads-per-core, and memory.\nMemory (RAM) This one is the easiest. Give enough. multiprocessing does not do a good job at clearning leaked memory. That needs to be handled explicitly in the job (del large_array). When memory spills happen, jobs crash (simple). You will soon realise what is enough. This is covered in more detail in the next article.\nCores and Threads This is confusing!\nRun the following command in the terminal: lscpu and look for these lines:\nThread(s) per core: If this is 2, hyper-threading is enabled. Core(s) per socket: Number of physical cores per CPU. CPU(s): Total logical cores (physical cores √ó threads per core Modern CPUs do hyperthreading, which allows them to run two threads per core by default. By not allowing the correct number of threads, we might block IO operations and end up crashing the process. For example, if you are reading a video file sequentially using cv2.VideoCapture, another ‚Äúthread‚Äù needs to be open all the time to read this file and run the FFMPEG backend for transcoding.\nTo lock these resources, we need to specifiy these flags:\n--ntasks: Number of tasks (processes) in a job. --cpus-per-task: CPUs (logical cores) allocated per task. --threads-per-core: Threads per physical core (default=1; set to 2 for hyper-threading). CPU Binding Use --cpu-bind to control how tasks/threads are bound to cores:\n--cpu-bind=cores: Bind each task to physical cores. --cpu-bind=threads: Bind to logical cores (hyper-threads). In case of confustion, add more than one core and more than one thread: (‚Äìcpus-per-task 2 ‚Äìthreads-per-core 2`).\nWalltime This also causes some crashes, especailly when running a lot of jobs. The default walltime for all dask workers is one hour. Increase it to something suitable (maximum allowed walltime for SLURM is 2 days).\nMonitor your usage directly from the terminal squeue -u $USER --format=\"%.18i %.9P %.8j %.8u %.2t %.10M %.6D %.4C %.4m %R\" `` 2. `dask-distributed` resource managements We are using `dask_jobqueue.SLURMCluster` wrapper to initalise a cluster on top of `SLURM` and use the `dask.distributed.Client` interface on top of it. This allow us to do the following: ```python ## All proper imports client = Client(SLURMCluster(*args, **kwargs)) client.scale(20) ## Request 20 jobs, each with the specified conditions def test(): import time time.sleep(5) return \"Finished\" ## Submit 100 `test`jobs on 20 \"dask-workers\"/\"slurm-jobs\" futures = [] for i in range(100): futures.append(client.submit(test)) Each of the worker behaves like a multiprocessing.Process, but with more problems because the interaction with the OS is more limited on job-scheduling systems. Let‚Äôs figure out how to properly manage resources on this dask.distributed interface.\nSeems to work:\nores=self.cores, processes=2, # 2 process per worker memory=self.memory, log_directory=self.log_dir, walltime=‚Äú5:00:00‚Äù ## Maximum walltime is 5 hours\n","wordCount":"733","inLanguage":"en","image":"https://yatharthb97.github.io/%3Cimage%20path/url%3E","datePublished":"2025-03-25T17:37:01Z","dateModified":"2025-03-25T17:37:01Z","author":{"@type":"Person","name":"Yatharth Bhasin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yatharthb97.github.io/notes/distributedcomputing/cluster/"},"publisher":{"@type":"Organization","name":"Yatharth Bhasin","logo":{"@type":"ImageObject","url":"https://yatharthb97.github.io/images/website_tile.svg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yatharthb97.github.io/ accesskey=h title="Yatharth Bhasin (Alt + H)"><img src=https://yatharthb97.github.io/images/header_button.gif alt aria-label=logo height=30>Yatharth Bhasin</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yatharthb97.github.io/aboutme/ title="about me"><span>about me</span></a></li><li><a href=https://yatharthb97.github.io/projects/ title=projects><span>projects</span></a></li><li><a href=https://yatharthb97.github.io/notes/ title=notes><span>notes</span></a></li><li><a href=https://yatharthb97.github.io/publications/ title=publications><span>publications</span></a></li><li><a href=https://yatharthb97.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://yatharthb97.github.io/gallery/ title=gallery><span>gallery</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yatharthb97.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://yatharthb97.github.io/notes/> ‚úíÔ∏è </a>&nbsp;¬ª&nbsp;<a href=https://yatharthb97.github.io/notes/distributedcomputing/>Distributed Computing (üñ•Ô∏è X10000)</a></div><h1 class=post-title>Cluster</h1><div class=post-description>Notes on how to set up a cluster.</div><div class=post-meta><span title='2025-03-25 17:37:01 +0000 UTC'>March 25, 2025</span>&nbsp;¬∑&nbsp;4 min&nbsp;¬∑&nbsp;733 words&nbsp;¬∑&nbsp;Yatharth Bhasin&nbsp;|&nbsp;<a href=https://github.com/yatharthb97/yatharthb97.github.io/tree/main/content//notes/distributedcomputing/cluster.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#1-slurm-resource-management aria-label="1. SLURM resource management">1. SLURM resource management</a><ul><li><a href=#memory-ram aria-label="Memory (RAM)">Memory (RAM)</a></li><li><a href=#cores-and-threads aria-label="Cores and Threads">Cores and Threads</a><ul><li><a href=#cpu-binding aria-label="CPU Binding">CPU Binding</a></li></ul></li><li><a href=#walltime aria-label=Walltime>Walltime</a></li><li><a href=#monitor-your-usage-directly-from-the-terminal aria-label="Monitor your usage directly from the terminal">Monitor your usage directly from the terminal</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>We will set-up a cluster using a job-scheduler (<code>SLURM</code>) and use <a href=https://distributed.dask.org/en/stable/><code>dask-distributed</code></a> to manage jobs. One does not normally choose the job-scheduler, it is given to us (like your institute or HPC service would use a specific one). It is for this reason that <code>dask</code> comes in handy. It implements a trivial-parallelisation model on top of most common job-schedulers (<code>SLURM</code>, <code>PBS</code>, etc) to name a few. The interface it provides resembles python&rsquo;s <code>multiprocessing</code> module.</p><p>Another benefit is that using <code>dask</code> native arrays and dataframes allow you to automatically manage memeory and work with &ldquo;larger-than-memory&rdquo; objects. However, working with these objects is tricky and especially complicated when you need to use libraries written in <code>C</code> like <code>python-opencv</code>. If you can get it to work, here is a good article that deals with a typical workflow: <a href=https://imaging.epfl.ch/field-guide/sections/performance_optimization/notebooks/performance_dask_image.html>here</a> and <a href=https://github.com/dask/dask-examples/blob/main/applications/image-processing.ipynb>here</a>.</p><p>We will only use it <code>dask-distributed</code> to schedule jobs on the fly using <code>jupyter-lab</code>. This approach allows us to do batch analysis before computation and allows us to compute parameters and send jobs on the fly. The interface is the same as <code>multiprocessing.fututes</code> and general <code>multiprocessing</code> rules are respected.</p><p>To efficiently compute on HPC machines using these two libraries, we need to understand:</p><ol><li><code>SLURM</code> resource managements</li><li><code>dask-distributed</code> resource managements</li><li>Patching and common pitfalls</li></ol><p>We will not cover aspects like installation and debugging here.</p><h2 id=1-slurm-resource-management>1. <code>SLURM</code> resource management<a hidden class=anchor aria-hidden=true href=#1-slurm-resource-management>#</a></h2><blockquote><p>User ‚Üí Submits ‚Üí Job ‚Üí Runs in ‚Üí Partition ‚Üí Contains ‚Üí Nodes<br>Job ‚Üí Spawns ‚Üí Steps<br>Job ‚Üí Uses ‚Üí Licenses<br>QoS ‚Üí Governs ‚Üí Job Limits<br>Reservation ‚Üí Reserves ‚Üí Nodes</p></blockquote><p>hence we know that each &ldquo;job&rdquo; can do several &ldquo;tasks&rdquo;:</p><pre tabindex=0><code>Jobs (With a JobID :: this corresponds to one dask worker)
|- Task1
|- Task2
:
|- TaskN
</code></pre><p>We need to properly allocate three things: physical-cpu-core, threads-per-core, and memory.</p><h3 id=memory-ram>Memory (RAM)<a hidden class=anchor aria-hidden=true href=#memory-ram>#</a></h3><p>This one is the easiest. Give enough. <code>multiprocessing</code> does not do a good job at clearning leaked memory. That needs to be handled explicitly in the job (<code>del large_array</code>). When memory spills happen, jobs crash (simple). You will soon realise what is enough. This is covered in more detail in the next article.</p><h3 id=cores-and-threads>Cores and Threads<a hidden class=anchor aria-hidden=true href=#cores-and-threads>#</a></h3><p>This is confusing!</p><p>Run the following command in the terminal: <code>lscpu</code> and look for these lines:</p><pre tabindex=0><code>Thread(s) per core: If this is 2, hyper-threading is enabled.
Core(s) per socket: Number of physical cores per CPU.
CPU(s): Total logical cores (physical cores √ó threads per core
</code></pre><p>Modern CPUs do hyperthreading, which allows them to run <strong>two</strong> threads per core by default. By not allowing the correct number of threads, we might block IO operations and end up crashing the process. For example, if you are reading a video file sequentially using <code>cv2.VideoCapture</code>, another &ldquo;thread&rdquo; needs to be open all the time to read this file and run the <code>FFMPEG</code> backend for transcoding.</p><p>To lock these resources, we need to specifiy these flags:</p><ul><li><code>--ntasks</code>: Number of tasks (processes) in a job.</li><li><code>--cpus-per-task</code>: CPUs (logical cores) allocated per task.</li><li><code>--threads-per-core</code>: Threads per physical core (default=1; set to 2 for hyper-threading).</li></ul><h4 id=cpu-binding>CPU Binding<a hidden class=anchor aria-hidden=true href=#cpu-binding>#</a></h4><p>Use <code>--cpu-bind</code> to control how tasks/threads are bound to cores:</p><ul><li><code>--cpu-bind=cores</code>: Bind each task to physical cores.</li><li><code>--cpu-bind=threads</code>: Bind to logical cores (hyper-threads).</li></ul><p>In case of confustion, add more than one core and more than one thread: (&ndash;cpus-per-task 2 &ndash;threads-per-core 2`).</p><h3 id=walltime>Walltime<a hidden class=anchor aria-hidden=true href=#walltime>#</a></h3><p>This also causes some crashes, especailly when running a lot of jobs. The default walltime for all dask workers is one hour. Increase it to something suitable (maximum allowed walltime for <code>SLURM</code> is 2 days).</p><h3 id=monitor-your-usage-directly-from-the-terminal>Monitor your usage directly from the terminal<a hidden class=anchor aria-hidden=true href=#monitor-your-usage-directly-from-the-terminal>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>squeue -u <span class=nv>$USER</span> --format<span class=o>=</span><span class=s2>&#34;%.18i %.9P %.8j %.8u %.2t %.10M %.6D %.4C %.4m %R&#34;</span>
</span></span><span class=line><span class=cl><span class=sb>``</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2. <span class=sb>`</span>dask-distributed<span class=sb>`</span> resource managements
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>We are using <span class=sb>`</span>dask_jobqueue.SLURMCluster<span class=sb>`</span> wrapper to initalise a  cluster on top of <span class=sb>`</span>SLURM<span class=sb>`</span> and use the <span class=sb>`</span>dask.distributed.Client<span class=sb>`</span> interface on top of it. This allow us to <span class=k>do</span> the following:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=sb>```</span>python
</span></span><span class=line><span class=cl><span class=c1>## All proper imports</span>
</span></span><span class=line><span class=cl><span class=nv>client</span> <span class=o>=</span> Client<span class=o>(</span>SLURMCluster<span class=o>(</span>*args, **kwargs<span class=o>))</span>
</span></span><span class=line><span class=cl>client.scale<span class=o>(</span>20<span class=o>)</span> <span class=c1>## Request 20 jobs, each with the specified conditions</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>def test<span class=o>()</span>:
</span></span><span class=line><span class=cl>    import <span class=nb>time</span>
</span></span><span class=line><span class=cl>    time.sleep<span class=o>(</span>5<span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=s2>&#34;Finished&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## Submit 100 `test`jobs on 20 &#34;dask-workers&#34;/&#34;slurm-jobs&#34;</span>
</span></span><span class=line><span class=cl><span class=nv>futures</span> <span class=o>=</span> <span class=o>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> i in range<span class=o>(</span>100<span class=o>)</span>:
</span></span><span class=line><span class=cl>    futures.append<span class=o>(</span>client.submit<span class=o>(</span><span class=nb>test</span><span class=o>))</span>
</span></span></code></pre></div><p>Each of the <code>worker</code> behaves like a <code>multiprocessing.Process</code>, but with more problems because the interaction with the OS is more limited on job-scheduling systems. Let&rsquo;s figure out how to properly manage resources on this <code>dask.distributed</code> interface.</p><hr><p>Seems to work:</p><p>ores=self.cores,
processes=2, # 2 process per worker
memory=self.memory,
log_directory=self.log_dir,
walltime=&ldquo;5:00:00&rdquo; ## Maximum walltime is 5 hours</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://yatharthb97.github.io/tags/blog/>Blog</a></li><li><a href=https://yatharthb97.github.io/tags/notes/>Notes</a></li></ul><nav class=paginav><a class=prev href=https://yatharthb97.github.io/notes/distributedcomputing/multiprocessing/><span class=title>¬´ Prev</span><br><span>Multiprocessing</span>
</a><a class=next href=https://yatharthb97.github.io/notes/biophy/urialonc9/><span class=title>Next ¬ª</span><br><span>Robustness in Bacterial Chemotaxis</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yatharthb97.github.io/>Yatharth Bhasin</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>